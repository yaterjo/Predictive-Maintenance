{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12993cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 220320 entries, 0 to 220319\n",
      "Data columns (total 55 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Unnamed: 0      220320 non-null  int64  \n",
      " 1   timestamp       220320 non-null  object \n",
      " 2   sensor_00       210112 non-null  float64\n",
      " 3   sensor_01       219951 non-null  float64\n",
      " 4   sensor_02       220301 non-null  float64\n",
      " 5   sensor_03       220301 non-null  float64\n",
      " 6   sensor_04       220301 non-null  float64\n",
      " 7   sensor_05       220301 non-null  float64\n",
      " 8   sensor_06       215522 non-null  float64\n",
      " 9   sensor_07       214869 non-null  float64\n",
      " 10  sensor_08       215213 non-null  float64\n",
      " 11  sensor_09       215725 non-null  float64\n",
      " 12  sensor_10       220301 non-null  float64\n",
      " 13  sensor_11       220301 non-null  float64\n",
      " 14  sensor_12       220301 non-null  float64\n",
      " 15  sensor_13       220301 non-null  float64\n",
      " 16  sensor_14       220299 non-null  float64\n",
      " 17  sensor_15       0 non-null       float64\n",
      " 18  sensor_16       220289 non-null  float64\n",
      " 19  sensor_17       220274 non-null  float64\n",
      " 20  sensor_18       220274 non-null  float64\n",
      " 21  sensor_19       220304 non-null  float64\n",
      " 22  sensor_20       220304 non-null  float64\n",
      " 23  sensor_21       220304 non-null  float64\n",
      " 24  sensor_22       220279 non-null  float64\n",
      " 25  sensor_23       220304 non-null  float64\n",
      " 26  sensor_24       220304 non-null  float64\n",
      " 27  sensor_25       220284 non-null  float64\n",
      " 28  sensor_26       220300 non-null  float64\n",
      " 29  sensor_27       220304 non-null  float64\n",
      " 30  sensor_28       220304 non-null  float64\n",
      " 31  sensor_29       220248 non-null  float64\n",
      " 32  sensor_30       220059 non-null  float64\n",
      " 33  sensor_31       220304 non-null  float64\n",
      " 34  sensor_32       220252 non-null  float64\n",
      " 35  sensor_33       220304 non-null  float64\n",
      " 36  sensor_34       220304 non-null  float64\n",
      " 37  sensor_35       220304 non-null  float64\n",
      " 38  sensor_36       220304 non-null  float64\n",
      " 39  sensor_37       220304 non-null  float64\n",
      " 40  sensor_38       220293 non-null  float64\n",
      " 41  sensor_39       220293 non-null  float64\n",
      " 42  sensor_40       220293 non-null  float64\n",
      " 43  sensor_41       220293 non-null  float64\n",
      " 44  sensor_42       220293 non-null  float64\n",
      " 45  sensor_43       220293 non-null  float64\n",
      " 46  sensor_44       220293 non-null  float64\n",
      " 47  sensor_45       220293 non-null  float64\n",
      " 48  sensor_46       220293 non-null  float64\n",
      " 49  sensor_47       220293 non-null  float64\n",
      " 50  sensor_48       220293 non-null  float64\n",
      " 51  sensor_49       220293 non-null  float64\n",
      " 52  sensor_50       143303 non-null  float64\n",
      " 53  sensor_51       204937 non-null  float64\n",
      " 54  machine_status  220320 non-null  object \n",
      "dtypes: float64(52), int64(1), object(2)\n",
      "memory usage: 92.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0            timestamp  sensor_00  sensor_01  sensor_02  \\\n",
       " 0           0  2018-04-01 00:00:00   2.465394   47.09201    53.2118   \n",
       " 1           1  2018-04-01 00:01:00   2.465394   47.09201    53.2118   \n",
       " 2           2  2018-04-01 00:02:00   2.444734   47.35243    53.2118   \n",
       " 3           3  2018-04-01 00:03:00   2.460474   47.09201    53.1684   \n",
       " 4           4  2018-04-01 00:04:00   2.445718   47.13541    53.2118   \n",
       " \n",
       "    sensor_03  sensor_04  sensor_05  sensor_06  sensor_07  ...  sensor_43  \\\n",
       " 0  46.310760   634.3750   76.45975   13.41146   16.13136  ...   41.92708   \n",
       " 1  46.310760   634.3750   76.45975   13.41146   16.13136  ...   41.92708   \n",
       " 2  46.397570   638.8889   73.54598   13.32465   16.03733  ...   41.66666   \n",
       " 3  46.397568   628.1250   76.98898   13.31742   16.24711  ...   40.88541   \n",
       " 4  46.397568   636.4583   76.58897   13.35359   16.21094  ...   41.40625   \n",
       " \n",
       "    sensor_44  sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  \\\n",
       " 0  39.641200   65.68287   50.92593  38.194440   157.9861   67.70834   \n",
       " 1  39.641200   65.68287   50.92593  38.194440   157.9861   67.70834   \n",
       " 2  39.351852   65.39352   51.21528  38.194443   155.9606   67.12963   \n",
       " 3  39.062500   64.81481   51.21528  38.194440   155.9606   66.84028   \n",
       " 4  38.773150   65.10416   51.79398  38.773150   158.2755   66.55093   \n",
       " \n",
       "    sensor_50  sensor_51  machine_status  \n",
       " 0   243.0556   201.3889          NORMAL  \n",
       " 1   243.0556   201.3889          NORMAL  \n",
       " 2   241.3194   203.7037          NORMAL  \n",
       " 3   240.4514   203.1250          NORMAL  \n",
       " 4   242.1875   201.3889          NORMAL  \n",
       " \n",
       " [5 rows x 55 columns],\n",
       " None,\n",
       "           Unnamed: 0      sensor_00      sensor_01      sensor_02  \\\n",
       " count  220320.000000  210112.000000  219951.000000  220301.000000   \n",
       " mean   110159.500000       2.372221      47.591611      50.867392   \n",
       " std     63601.049991       0.412227       3.296666       3.666820   \n",
       " min         0.000000       0.000000       0.000000      33.159720   \n",
       " 25%     55079.750000       2.438831      46.310760      50.390620   \n",
       " 50%    110159.500000       2.456539      48.133678      51.649300   \n",
       " 75%    165239.250000       2.499826      49.479160      52.777770   \n",
       " max    220319.000000       2.549016      56.727430      56.032990   \n",
       " \n",
       "            sensor_03      sensor_04      sensor_05      sensor_06  \\\n",
       " count  220301.000000  220301.000000  220301.000000  215522.000000   \n",
       " mean       43.752481     590.673936      73.396414      13.501537   \n",
       " std         2.418887     144.023912      17.298247       2.163736   \n",
       " min        31.640620       2.798032       0.000000       0.014468   \n",
       " 25%        42.838539     626.620400      69.976260      13.346350   \n",
       " 50%        44.227428     632.638916      75.576790      13.642940   \n",
       " 75%        45.312500     637.615723      80.912150      14.539930   \n",
       " max        48.220490     800.000000      99.999880      22.251160   \n",
       " \n",
       "            sensor_07      sensor_08  ...      sensor_42      sensor_43  \\\n",
       " count  214869.000000  215213.000000  ...  220293.000000  220293.000000   \n",
       " mean       15.843152      15.200721  ...      35.453455      43.879591   \n",
       " std         2.201155       2.037390  ...      10.259521      11.044404   \n",
       " min         0.000000       0.028935  ...      22.135416      24.479166   \n",
       " 25%        15.907120      15.183740  ...      32.812500      39.583330   \n",
       " 50%        16.167530      15.494790  ...      35.156250      42.968750   \n",
       " 75%        16.427950      15.697340  ...      36.979164      46.614580   \n",
       " max        23.596640      24.348960  ...     374.218800     408.593700   \n",
       " \n",
       "            sensor_44      sensor_45      sensor_46      sensor_47  \\\n",
       " count  220293.000000  220293.000000  220293.000000  220293.000000   \n",
       " mean       42.656877      43.094984      48.018585      44.340903   \n",
       " std        11.576355      12.837520      15.641284      10.442437   \n",
       " min        25.752316      26.331018      26.331018      27.199070   \n",
       " 25%        36.747684      36.747684      40.509258      39.062500   \n",
       " 50%        40.509260      40.219910      44.849540      42.534720   \n",
       " 75%        45.138890      44.849540      51.215280      46.585650   \n",
       " max      1000.000000     320.312500     370.370400     303.530100   \n",
       " \n",
       "            sensor_48      sensor_49      sensor_50      sensor_51  \n",
       " count  220293.000000  220293.000000  143303.000000  204937.000000  \n",
       " mean      150.889044      57.119968     183.049260     202.699667  \n",
       " std        82.244957      19.143598      65.258650     109.588607  \n",
       " min        26.331018      26.620370      27.488426      27.777779  \n",
       " 25%        83.912030      47.743060     167.534700     179.108800  \n",
       " 50%       138.020800      52.662040     193.865700     197.338000  \n",
       " 75%       208.333300      60.763890     219.907400     216.724500  \n",
       " max       561.632000     464.409700    1000.000000    1000.000000  \n",
       " \n",
       " [8 rows x 53 columns])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'sensor.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "data.head(), data.info(), data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd93f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop unnecessary columns\n",
    "data_cleaned = data.drop(columns=['Unnamed: 0', 'timestamp', 'sensor_15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64759633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORMAL        205836\n",
       "RECOVERING     14477\n",
       "BROKEN             7\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Check the distribution of 'machine_status' to understand its format for classification\n",
    "machine_status_distribution = data_cleaned['machine_status'].value_counts()\n",
    "\n",
    "# Display the distribution of 'machine_status'\n",
    "machine_status_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a01b95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BROKEN', 'NORMAL', 'RECOVERING']\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data_cleaned['machine_status'] = le.fit_transform(data_cleaned['machine_status'])\n",
    "\n",
    "# Get the encoded class labels\n",
    "encoded_classes = le.classes_.tolist()  # Convert to list\n",
    "print(encoded_classes)  # This should print ['BROKEN', 'NORMAL', 'RECOVERING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf876db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119103, 52)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Remove rows with missing values\n",
    "data_cleaned = data_cleaned.dropna()\n",
    "\n",
    "# Confirm the removal and prepare for the next steps by showing the updated shape of the dataset\n",
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9e76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into features and labels\n",
    "X = data_cleaned.drop('machine_status', axis=1)\n",
    "y = data_cleaned['machine_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "230583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4f5d548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83372, 51), (35731, 51), (83372,), (35731,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of the training and testing sets to confirm\n",
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d0945c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9998320785872212\n",
      "Precision: 0.6662374640098139\n",
      "Recall: 0.6655155501770188\n",
      "F1 Score: 0.6658760716164246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming all other columns are numerical after cleaning and excluding the target 'machine_status'\n",
    "numerical_features = [col for col in data_cleaned.columns if col != 'machine_status']\n",
    "\n",
    "# Define preprocessing for numerical columns (scaling, imputing)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features)\n",
    "])\n",
    "\n",
    "# Custom class weights based on provided distribution\n",
    "class_weights = {0: 1,       # NORMAL\n",
    "                 1: 100,     # RECOVERING\n",
    "                 2: 10000}   # BROKEN\n",
    "\n",
    "# Create a preprocessing and modeling pipeline with custom class weights\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(class_weight=class_weights, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to your training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3981917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Recall Scores: [0.66496597 0.66665232 0.66440386 0.66498033 0.66216976]\n",
      "Average CV Recall: 0.6646344472290769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "# Define the cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# We'll use recall as the scoring metric to focus on the minority class performance\n",
    "recall_scorer = make_scorer(recall_score, average='macro', zero_division=0)\n",
    "\n",
    "# Conducting the cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=stratified_kfold, scoring=recall_scorer)\n",
    "\n",
    "print(f\"CV Recall Scores: {cv_scores}\")\n",
    "print(f\"Average CV Recall: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eaa311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'classifier__class_weight': 'balanced', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Best Recall Score: 0.6661165165456332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters to tune\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "    'classifier__class_weight': [{0: 1, 1: 100, 2: 10000}, 'balanced']\n",
    "}\n",
    "\n",
    "# Setting up GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring=recall_scorer, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fitting GridSearchCV\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Recall Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a2b3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2383/2383 [==============================] - 2s 909us/step - loss: -21278.2910 - accuracy: 0.9728 - val_loss: -75828.6953 - val_accuracy: 0.9758\n",
      "Epoch 2/30\n",
      "2383/2383 [==============================] - 2s 846us/step - loss: -228217.1094 - accuracy: 0.9750 - val_loss: -423108.7500 - val_accuracy: 0.9758\n",
      "Epoch 3/30\n",
      "2383/2383 [==============================] - 9s 4ms/step - loss: -773043.3125 - accuracy: 0.9750 - val_loss: -1133796.2500 - val_accuracy: 0.9758\n",
      "Epoch 4/30\n",
      "2383/2383 [==============================] - 5s 2ms/step - loss: -1723230.8750 - accuracy: 0.9750 - val_loss: -2289479.5000 - val_accuracy: 0.9758\n",
      "Epoch 5/30\n",
      "2383/2383 [==============================] - 5s 2ms/step - loss: -3184227.7500 - accuracy: 0.9750 - val_loss: -3972369.5000 - val_accuracy: 0.9758\n",
      "Epoch 6/30\n",
      "2383/2383 [==============================] - 5s 2ms/step - loss: -5253623.0000 - accuracy: 0.9750 - val_loss: -6290151.0000 - val_accuracy: 0.9758\n",
      "Epoch 7/30\n",
      "2383/2383 [==============================] - 2s 920us/step - loss: -7784880.5000 - accuracy: 0.9750 - val_loss: -8944060.0000 - val_accuracy: 0.9758\n",
      "Epoch 8/30\n",
      "2383/2383 [==============================] - 2s 997us/step - loss: -11029217.0000 - accuracy: 0.9750 - val_loss: -12521723.0000 - val_accuracy: 0.9758\n",
      "Epoch 9/30\n",
      "2383/2383 [==============================] - 2s 1ms/step - loss: -15160134.0000 - accuracy: 0.9750 - val_loss: -16966844.0000 - val_accuracy: 0.9758\n",
      "Epoch 10/30\n",
      "2383/2383 [==============================] - 2s 835us/step - loss: -20222410.0000 - accuracy: 0.9750 - val_loss: -22344582.0000 - val_accuracy: 0.9758\n",
      "Epoch 11/30\n",
      "2383/2383 [==============================] - 2s 778us/step - loss: -26365752.0000 - accuracy: 0.9750 - val_loss: -28849678.0000 - val_accuracy: 0.9758\n",
      "Epoch 12/30\n",
      "2383/2383 [==============================] - 2s 842us/step - loss: -33558936.0000 - accuracy: 0.9750 - val_loss: -36294284.0000 - val_accuracy: 0.9758\n",
      "Epoch 13/30\n",
      "2383/2383 [==============================] - 2s 787us/step - loss: -41905188.0000 - accuracy: 0.9750 - val_loss: -44968452.0000 - val_accuracy: 0.9758\n",
      "Epoch 14/30\n",
      "2383/2383 [==============================] - 2s 785us/step - loss: -51466052.0000 - accuracy: 0.9750 - val_loss: -54815648.0000 - val_accuracy: 0.9758\n",
      "Epoch 15/30\n",
      "2383/2383 [==============================] - 2s 793us/step - loss: -62312832.0000 - accuracy: 0.9750 - val_loss: -66006680.0000 - val_accuracy: 0.9758\n",
      "Epoch 16/30\n",
      "2383/2383 [==============================] - 2s 800us/step - loss: -74744984.0000 - accuracy: 0.9750 - val_loss: -78714232.0000 - val_accuracy: 0.9758\n",
      "Epoch 17/30\n",
      "2383/2383 [==============================] - 2s 788us/step - loss: -88409056.0000 - accuracy: 0.9750 - val_loss: -92579616.0000 - val_accuracy: 0.9758\n",
      "Epoch 18/30\n",
      "2383/2383 [==============================] - 2s 826us/step - loss: -103637728.0000 - accuracy: 0.9750 - val_loss: -108157728.0000 - val_accuracy: 0.9758\n",
      "Epoch 19/30\n",
      "2383/2383 [==============================] - 2s 857us/step - loss: -120585208.0000 - accuracy: 0.9750 - val_loss: -125475928.0000 - val_accuracy: 0.9758\n",
      "Epoch 20/30\n",
      "2383/2383 [==============================] - 2s 877us/step - loss: -139458896.0000 - accuracy: 0.9750 - val_loss: -144382848.0000 - val_accuracy: 0.9758\n",
      "Epoch 21/30\n",
      "2383/2383 [==============================] - 2s 856us/step - loss: -159881936.0000 - accuracy: 0.9750 - val_loss: -165256976.0000 - val_accuracy: 0.9758\n",
      "Epoch 22/30\n",
      "2383/2383 [==============================] - 2s 865us/step - loss: -182226544.0000 - accuracy: 0.9750 - val_loss: -187688848.0000 - val_accuracy: 0.9758\n",
      "Epoch 23/30\n",
      "2383/2383 [==============================] - 2s 866us/step - loss: -206668336.0000 - accuracy: 0.9750 - val_loss: -212475936.0000 - val_accuracy: 0.9758\n",
      "Epoch 24/30\n",
      "2383/2383 [==============================] - 2s 854us/step - loss: -233197408.0000 - accuracy: 0.9750 - val_loss: -239057728.0000 - val_accuracy: 0.9758\n",
      "Epoch 25/30\n",
      "2383/2383 [==============================] - 2s 853us/step - loss: -261974080.0000 - accuracy: 0.9750 - val_loss: -268014176.0000 - val_accuracy: 0.9758\n",
      "Epoch 26/30\n",
      "2383/2383 [==============================] - 2s 857us/step - loss: -292694304.0000 - accuracy: 0.9750 - val_loss: -298607808.0000 - val_accuracy: 0.9758\n",
      "Epoch 27/30\n",
      "2383/2383 [==============================] - 2s 861us/step - loss: -325541024.0000 - accuracy: 0.9750 - val_loss: -331672768.0000 - val_accuracy: 0.9758\n",
      "Epoch 28/30\n",
      "2383/2383 [==============================] - 2s 870us/step - loss: -361025568.0000 - accuracy: 0.9750 - val_loss: -367069696.0000 - val_accuracy: 0.9758\n",
      "Epoch 29/30\n",
      "2383/2383 [==============================] - 2s 853us/step - loss: -398157312.0000 - accuracy: 0.9750 - val_loss: -403767200.0000 - val_accuracy: 0.9758\n",
      "Epoch 30/30\n",
      "2383/2383 [==============================] - 2s 856us/step - loss: -437975968.0000 - accuracy: 0.9750 - val_loss: -443719872.0000 - val_accuracy: 0.9758\n",
      "745/745 - 0s - loss: -4.5791e+08 - accuracy: 0.9748\n",
      "\n",
      "Test accuracy: 0.9748121500015259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming 'data_cleaned' is your cleaned DataFrame\n",
    "X = data_cleaned[numerical_features]\n",
    "y = data_cleaned['machine_status']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numerical columns (imputation and scaling)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply preprocessing to numerical features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features)\n",
    "])\n",
    "\n",
    "# Assuming the target variable 'machine_status' is binary and already properly encoded\n",
    "\n",
    "# Define a simple Feedforward Neural Network model for binary classification\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(len(numerical_features),)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Since we're using a neural network, we need to fit the model on scaled and imputed data\n",
    "# First, apply the preprocessing to the training and testing data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Now, fit the neural network model\n",
    "history = model.fit(X_train_preprocessed, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the neural network model\n",
    "test_loss, test_acc = model.evaluate(X_test_preprocessed, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5021b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745/745 - 0s - loss: -4.5791e+08 - accuracy: 0.9748 - precision: 1.0000 - recall: 1.0000\n",
      "\n",
      "Test accuracy: 0.9748121500015259\n",
      "Test precision: 0.9999580383300781\n",
      "Test recall: 1.0\n",
      "Test F1 Score: 0.9999790187248344\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# When compiling the model, add additional metrics\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "# Evaluate the model to include these metrics\n",
    "loss, accuracy, precision, recall = model.evaluate(X_test_preprocessed, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {accuracy}')\n",
    "print(f'Test precision: {precision}')\n",
    "print(f'Test recall: {recall}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'Test F1 Score: {f1_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c56deeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[    0     1     0]\n",
      " [    0 23221     0]\n",
      " [    0   599     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predict classes on the test set\n",
    "y_pred_classes = (model.predict(X_test_preprocessed) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f542b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-b7f1962f4d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Split the dataset, ensuring stratification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Normalize features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2439\u001b[0m         \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2441\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \"\"\"\n\u001b[1;32m-> 2022\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2023\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    112\u001b[0m         ):\n\u001b[0;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"infinity\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"NaN, infinity\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Assuming 'data_cleaned' is preprocessed and 'numerical_features' are defined\n",
    "X = data_cleaned[numerical_features]\n",
    "y = data_cleaned['machine_status'].map({'NORMAL': 0, 'RECOVERING': 1, 'BROKEN': 2})  # Encoding classes as integers\n",
    "\n",
    "# Split the dataset, ensuring stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training set for dealing with imbalance\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # k_neighbors set to 1 due to low number of BROKEN samples\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Calculate class weights based on the original class distribution\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# It's crucial to compute class weights based on the original distribution before SMOTE\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i : class_weights[i] for i in range(3)}\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_smote, y_train_smote, epochs=30, batch_size=32, class_weight=class_weights_dict, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4eadd2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_smote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6d5b5496c51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Train the model with early stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m history = model.fit(X_train_smote, y_train_smote, epochs=30, batch_size=32,\n\u001b[0m\u001b[0;32m     13\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weights_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     callbacks=[early_stopping])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_smote' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Adding early stopping to monitor validation loss and prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Including additional metrics for a comprehensive evaluation\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_smote, y_train_smote, epochs=30, batch_size=32,\n",
    "                    class_weight=class_weights_dict, validation_split=0.2,\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30458110",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b16a2532236b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[0;32m   2421\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2098\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2099\u001b[0m             \u001b[1;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m             \u001b[1;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the normalized test set\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "print(f'Test Precision: {test_precision}')\n",
    "print(f'Test Recall: {test_recall}')\n",
    "\n",
    "# Calculate and print the F1 score\n",
    "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "print(f'Test F1 Score: {f1_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "800f4ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7cca38208873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Split the dataset, ensuring stratification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Normalize features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2439\u001b[0m         \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2441\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \"\"\"\n\u001b[1;32m-> 2022\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2023\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    112\u001b[0m         ):\n\u001b[0;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"infinity\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"NaN, infinity\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'data_cleaned' is preprocessed and 'numerical_features' are defined\n",
    "X = data_cleaned[numerical_features]\n",
    "y = data_cleaned['machine_status'].map({'NORMAL': 0, 'RECOVERING': 1, 'BROKEN': 2})  # Encoding classes as integers\n",
    "\n",
    "# Split the dataset, ensuring stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training set for dealing with imbalance\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # k_neighbors set to 1 due to low number of BROKEN samples\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Calculate class weights based on the original class distribution\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# It's crucial to compute class weights based on the original distribution before SMOTE\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i : class_weights[i] for i in range(3)}\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_smote, y_train_smote, epochs=30, batch_size=32, class_weight=class_weights_dict, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0b008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
